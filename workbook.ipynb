{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modules\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "from data import data_loader\n",
    "from data import gesla_preprocessing\n",
    "from data import era5_preprocessing\n",
    "from data import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work if theres time\n",
    "\n",
    "1. Describe all modules in .py file <br>\n",
    "\n",
    "2. Do we need to minimize risk of misclassifcation (see Olli>Lernverfahren>05_classification)? <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "1. How do we optimize automatic search? <br>\n",
    "\n",
    "2. Function that saves test and trainstatistics during modelrun to analyze overfitting. \n",
    "\n",
    "3. Use Learning Curves to analyze overfitting during fitting process of hyperparameters?\n",
    "\n",
    "4. Evaluation module <br>\n",
    "\n",
    "4.1 Confusion Matrix (done) <br> \n",
    "\n",
    "4.2 Error rate (#errors / #instances) --> Success rate / accuracy = 1 - error rate <br>\n",
    "\n",
    "5. Do I need a validation set? Now I do have train-test datasets. But one could use train, validation and testsets.\n",
    "\n",
    "6. When using combined predictors, I need to normalize features, right (see Scaling in ML-Course Udemy)? I did not do that in previous runs (rf005, rf006)\n",
    "\n",
    "7. Use Gradient boosting? (see meta learning, Lernverfahren)\n",
    "\n",
    "8. Regression using Random Forests\n",
    "\n",
    "9. Use SVM with Kernel for classification?\n",
    "\n",
    "10. Add Pipeline to GridSearchCV?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---\n",
    "# Scale data\n",
    "#---\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "s = StandardScaler()\n",
    "s.fit(X_train)\n",
    "X_train = s.transform(X_train)\n",
    "X_test = s.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---\n",
    "# Learning Curve\n",
    "#---\n",
    "\n",
    "#- Train & Test split\n",
    "X = df.drop('success', axis = 1).values #- Copies DF\n",
    "y = df['success'].values\n",
    "\n",
    "#- Plot Learning Curve\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "\n",
    "X, y = shuffle(X, y) #- random selection of data. Good if you dont know if data is ordered\n",
    "train_sizes_abs, train_scores, test_scores = learning_curve(LogisticRegression(), X, y)\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_sizes_abs, np.mean(train_scores, axis = 1)) #- learning curve macht automatisch k-fold crossvalidation. deswegen mean\n",
    "plt.plot(train_sizes_abs, np.mean(test_scores, axis = 1)) #- learning curve macht automatisch k-fold crossvalidation. deswegen mean\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#- Note: Do this more often to get mean. Sometimes Curves look weird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---\n",
    "# Pipeline GridSearchCV: \n",
    "# Add this to modelfit.py?\n",
    "#---\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC \n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svc\", SVC()),\n",
    "])\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = GridSearchCV(pipeline, param_grid = {\n",
    "    \"svc__C\": [0.001, 0.01, 0.1, 1, 10,],\n",
    "    \"svc__gamma\": [0.001, 0.01, 0.1, 1, 10,],\n",
    "})\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(clf.best_params_)\n",
    "\n",
    "print(clf.score(X_validation, y_validation)) # Accuracy on basis of test data\n",
    "\n",
    "print(clf.best_score_) # Accuracy based on k-fold cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modularized Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---\n",
    "# Modularize Preprocessing\n",
    "#---\n",
    "\n",
    "# Get timeseries of predictor and predictand\n",
    "season = \"winter\" # [\"winter\", \"autumn\",] \n",
    "predictors = [\"sp\", \"tp\", \"u10\", \"v10\",]\n",
    "percentile = 0.95 # [0.95, 0.99,] \n",
    "preprocess = \"preprocess1\" # [\"preprocess1\"]\n",
    "range_of_years = \"1999-2008\" # [\"1999-2008\", \"2009-2018\", \"2019-2022\",]\n",
    "subregion = \"lon-0530_lat7040\" # [\"lon-0530_lat7040\"]\n",
    "station_names = [\"hanko-han-fin-cmems\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(903, 121, 141)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load already preprocessed Era5 Data\n",
    "# Preprocessing done with cdo\n",
    "#---\n",
    "predictor = predictors[0]\n",
    "era5_predictor = data_loader.load_daymean_era5(range_of_years, subregion, season, predictor, preprocess)\n",
    "\n",
    "era5_predictor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Predictand from GESLA\n",
      "Applied one-hot-encoding with Percentile: 0.95\n"
     ]
    }
   ],
   "source": [
    "#---\n",
    "# Preprocess GESLA Data\n",
    "#---\n",
    "\n",
    "# Load Predictand\n",
    "#---\n",
    "gesla_predictand = data_loader.load_gesla(station_names)\n",
    "\n",
    "# Select a season\n",
    "#---\n",
    "gesla_predictand = gesla_preprocessing.select_season(gesla_predictand, season)\n",
    "\n",
    "# Select only sea_level analysis data\n",
    "#---\n",
    "gesla_predictand = gesla_preprocessing.get_analysis(gesla_predictand)\n",
    "\n",
    "# Subtract mean of data grouped by station\n",
    "#---\n",
    "gesla_predictand = gesla_predictand[\"sea_level\"] # Detrend expects pd.Series\n",
    "gesla_predictand = gesla_preprocessing.detrend(gesla_predictand, level=\"station\")\n",
    "\n",
    "# Apply one hot encoding\n",
    "gesla_predictand = gesla_preprocessing.apply_dummies(gesla_predictand, percentile=percentile, level=\"station\")\n",
    "print(f\"Applied one-hot-encoding with Percentile: {percentile}\")\n",
    "\n",
    "# Convert to DataArray\n",
    "# nan values: no measurement at that timestamp for specific station\n",
    "gesla_predictand = gesla_predictand.to_xarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get overlapping timeseries of ERA5 and GESLA\n",
      "X: (903, 121, 141)\n",
      "Y: (903, 1)\n",
      "t: (903,)\n"
     ]
    }
   ],
   "source": [
    "#---\n",
    "# Get overlapping time-series\n",
    "#---\n",
    "X, Y, t = preprocessing.intersect_time(era5_predictor, gesla_predictand)\n",
    "\n",
    "print(f\"X: {X.shape}\")\n",
    "print(f\"Y: {Y.shape}\")\n",
    "print(f\"t: {t.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(903, 17061)\n",
      "(903,)\n"
     ]
    }
   ],
   "source": [
    "# Reshape for model input\n",
    "#---\n",
    "ndim = t.shape[0]\n",
    "X = X.reshape(ndim, -1) # (ndim, nclasses)\n",
    "y = Y[:, 0] # Select one station\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---\n",
    "# Handle NaN Values\n",
    "#---\n",
    "\n",
    "# Insert numerical value that is not in data.\n",
    "# ML will hopefully recognize it.\n",
    "X[np.where(np.isnan(X))] = -999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modularized Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(677, 17061)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#---\n",
    "# Train Model\n",
    "#---\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.25)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tested Hyperparameters: {'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000], 'max_features': ['auto', 'sqrt'], 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None], 'min_samples_split': [2, 5, 10], 'min_samples_leaf': [1, 2, 4], 'bootstrap': [True, False]}\n",
      "Optimize Hyperparameters using RandomSearchCV\n",
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    }
   ],
   "source": [
    "#---\n",
    "#  Optimize Hyperparameters\n",
    "#---\n",
    "from models import modelfit\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "optimizer = \"RandomSearchCV\"\n",
    "k = 3 # k-fold cross-validation\n",
    "n_iter = 100 # number of combinations\n",
    "\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(erx) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "param_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "print(f\"Tested Hyperparameters: {param_grid}\")\n",
    "\n",
    "best_params = modelfit.optimize_hyperparameter(X_train, y_train, clf, optimizer, param_grid, k, n_iter=n_iter, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---\n",
    "#  Save hyperparameters\n",
    "#---\n",
    "from data import saver\n",
    "\n",
    "model_run = f\"rf004\"\n",
    "folder = \"results/random_forest/\" \n",
    "saver.save_hpdict(best_params, predictor, model_run, percentile, folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(class_weight='balanced', max_depth=20,\n",
       "                       max_features='sqrt', min_samples_leaf=2,\n",
       "                       n_estimators=600, oob_score=True, random_state=0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#---\n",
    "# Fit the model\n",
    "#---\n",
    "model = RandomForestClassifier(criterion='gini',\n",
    "n_estimators=best_params[\"n_estimators\"], #- nTrees \n",
    "max_depth=best_params[\"max_depth\"], \n",
    "max_features=best_params[\"max_features\"],\n",
    "min_samples_leaf=best_params[\"min_samples_leaf\"],\n",
    "min_samples_split=best_params[\"min_samples_split\"],\n",
    "bootstrap=best_params[\"bootstrap\"],\n",
    "random_state=0, # To compare results when changing hyperparameters\n",
    "class_weight=\"balanced\",\n",
    "oob_score=True,\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---\n",
    "# Saving the model\n",
    "#---\n",
    "filename = f'RandomForest_{optimizer}.sav'\n",
    "pickle.dump(model, open(f'{folder}{filename}', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method BaseEstimator.get_params of RandomForestClassifier(class_weight='balanced', max_depth=20,\n",
       "                       max_features='sqrt', min_samples_leaf=2,\n",
       "                       n_estimators=600, oob_score=True, random_state=0)>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#---\n",
    "# Load the model from disk\n",
    "#---\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "result = loaded_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---\n",
    "# Evaluate model / Diagnostic\n",
    "#--- \n",
    "print(\"Evaluate Model \\n\")\n",
    "\n",
    "# Score & Importance\n",
    "#---\n",
    "test_score = model.score(X_test, y_test)\n",
    "train_score = model.score(X_train, y_train)\n",
    "importance = model.feature_importances_\n",
    "\n",
    "print(f\"test_score: {test_score}\")\n",
    "print(f\"train_score: {train_score}\")\n",
    "print(f\"importance: {importance}\")\n",
    "\n",
    "fname = f\"importance_{predictor}{str(percentile)[-2:]}\"\n",
    "np.save(f\"{folder}{fname}\", importance)\n",
    "print(f\"saved importance to : {folder}{fname}\")\n",
    "\n",
    "# Confusion matrix\n",
    "#---\n",
    "# Format: \n",
    "# Reality / Model: Negative, Positive\n",
    "# Negative    Right Negative, False Positive \n",
    "# Positive    False Negative, Right Positive\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from models import evaluation\n",
    "\n",
    "print(\"Show Confusion Matrix \\n\")\n",
    "\n",
    "cfm_fig = evaluation.plot_cf(model, X_test, y_test)\n",
    "cfm_fig.show()\n",
    "\n",
    "# Save CFM\n",
    "fname = f\"{folder}cf_matrix_{predictor}{str(percentile)[-2:]}.jpg\"\n",
    "cfm_fig.savefig(fname)\n",
    "print(f\"saved cf matrix to : {fname}\")\n",
    "\n",
    "# Calculate CFM-Metrics\n",
    "metrics = evaluation.cfm_metrics(model, X_test, y_test)\n",
    "fname = f\"cf_metrics_{predictor}{str(percentile)[-2:]}.pkl\"\n",
    "\n",
    "with open(f\"{folder}{fname}\", 'wb') as f:\n",
    "    pickle.dump(metrics, f)\n",
    "\n",
    "print(f\"saved cf metrics to : {fname}\")\n",
    "\n",
    "\n",
    "# AUROC\n",
    "# Receiver Operating Characteristics & Area Under the Curve\n",
    "#---\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Show AUROC \\n\")\n",
    "\n",
    "y_test_proba = model.predict_proba(X_test)[:, 1] # Prob. for predicting 0 or 1, we only need second col\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_test_proba)\n",
    "auc = roc_auc_score(y_test, y_test_proba)\n",
    "\n",
    "print(f'AUC: {auc}')\n",
    "\n",
    "fig, ax = plt.subplots(tight_layout=True)\n",
    "\n",
    "ax.plot(fpr, tpr)\n",
    "ax.set_xlabel(\"FPR\")\n",
    "ax.set_ylabel(\"TPR\")\n",
    "ax.set_title(f\"AUROC with AUC = {auc}\")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "fname = f\"{folder}AUROC_{predictor}{str(percentile)[-2:]}.jpg\"\n",
    "fig.savefig(fname)\n",
    "print(f\"saved AUROC to : {fname}\")\n",
    "\n",
    "#---\n",
    "# Visualize importance maps\n",
    "#---\n",
    "# Load lat lons\n",
    "lats, lons = preprocessing.get_lonlats(\n",
    "    range_of_years,\n",
    "    subregion,\n",
    "    season,\n",
    "    predictor,\n",
    "    preprocess,\n",
    ")\n",
    "\n",
    "# Plot importance-map\n",
    "from models import evaluation\n",
    "\n",
    "tflag = f\"{predictor}{str(percentile)[-2:]}\"\n",
    "\n",
    "fig = evaluation.importance_map(importance, lons, lats, tflag=tflag)\n",
    "\n",
    "# Save importance-map\n",
    "folder = f\"results/random_forest/{model_run}/\"\n",
    "fname = f\"importance_map_{predictor}{str(percentile)[-2:]}\"\n",
    "fig.savefig(f\"{folder}{fname}.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not Modularized Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load ERA5-Predictor: sp in region: lon-0530_lat7040 for years: 1999-2008 in season: winter\n",
      "Load Predictand from GESLA\n",
      "Applied one-hot-encoding\n",
      "Get overlapping timeseries of ERA5 and GESLA\n"
     ]
    }
   ],
   "source": [
    "# ---\n",
    "# Preprocessing\n",
    "# ---\n",
    "\n",
    "model_run = \"rf002\"\n",
    "\n",
    "# Get timeseries of predictor and predictand\n",
    "percentile = 0.95\n",
    "predictors = [\"sp\", \"tp\", \"u10\", \"v10\",]\n",
    "predictor = \"sp\"\n",
    "season = \"winter\"\n",
    "\n",
    "\n",
    "X, Y, t = preprocessing.preprocessing1(season, predictor, percentile)\n",
    "\n",
    "# Handle NaN values: \n",
    "# Insert numerical value that is not in data.\n",
    "# ML will hopefully recognize it.\n",
    "X[np.where(np.isnan(X))] = -999\n",
    "\n",
    "# Save number of lat/lon for interpreting model output later\n",
    "ndim = X.shape[0]\n",
    "nlat = X.shape[1]\n",
    "nlon = X.shape[2]\n",
    "\n",
    "# Prepare shape for model\n",
    "X = X.reshape(ndim, -1) # (ndim, nclasses)\n",
    "y = Y[:, 0] # Select only one station"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---\n",
    "# Load Hyperparameters rf002\n",
    "#---\n",
    "\n",
    "import pickle\n",
    "\n",
    "predictors = [\"sp\", \"tp\", \"u10\", \"v10\",]\n",
    "model_run = \"rf002\"\n",
    "percentile = 0.99\n",
    "pflag = str(percentile)[-2:]\n",
    "folder = f\"models/random_forest/{model_run}/\"\n",
    "\n",
    "print(f\"model_run: {model_run}, percentile: {percentile}\")\n",
    "for predictor in predictors:\n",
    "    fname = f\"{model_run}_{predictor}{pflag}.pkl\"\n",
    "\n",
    "    with open(f\"{folder}{fname}\", 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    print(predictor)\n",
    "    print(data)\n",
    "\n",
    "#---\n",
    "# Hyperparameters rf003\n",
    "#---\n",
    "model_run = \"rf003\"\n",
    "folder = f\"models/random_forest/{model_run}/\"\n",
    "\n",
    "print(f\"model_run: {model_run}, percentile: {percentile}\")\n",
    "for predictor in predictors:\n",
    "    fname = f\"{model_run}_{predictor}{pflag}.pkl\"\n",
    "\n",
    "    with open(f\"{folder}{fname}\", 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    print(predictor)\n",
    "    print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Postprocessing"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dbda0df39e8d9f08c77def502e386bb54b10e33e74098d5a47640c70cb49662a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('mlpy38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
