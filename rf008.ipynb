{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: numpy.ndarray size changed, may indicate binary incompatibility. Expected 16 from C header, got 88 from PyObject\n"
     ]
    }
   ],
   "source": [
    "# Modules\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "from data import data_loader\n",
    "from data import gesla_preprocessing\n",
    "from data import era5_preprocessing\n",
    "from data import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description\n",
    "\n",
    "- Rerun rf004 with optimized Hyperparameter ranges and scaled (!) predictors\n",
    "- Use new savings of metrics, e.g. confusion matrix for train-set, relative accuracy etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "- Modularize the hyperparameter and modelfit optimization, s.t. I can choose the classifier for each run and give a param grid to the run depending on the classifier (done)\n",
    "\n",
    "- Check if folders are there when creating it (done)\n",
    "\n",
    "- Check if GridSearchCV or RandomSearchCV need StandardScaler() as argument to scale the training data! Very important! Maybe I need to use Pipelines for this and implement the Pipeline in the fitting module. (I scaled the data beforehand, that should be sufficient). (done)\n",
    "\n",
    "- implement this: use_params (dict): Dictionary of hyperparameters that should be used for model. If None, hyperparameters are optimized. (Defaults: None). done\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Predictand from GESLA\n",
      "Applied one-hot-encoding with Percentile: 0.95\n",
      "Add predictor sp to model input features\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\OneDrive\\Alle_Ordner\\Coding\\Python\\master_thesis\\rf008.ipynb Zelle 5\u001b[0m in \u001b[0;36m<cell line: 46>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive/Alle_Ordner/Coding/Python/master_thesis/rf008.ipynb#ch0000014?line=39'>40</a>\u001b[0m hparam_grid \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mn_estimators\u001b[39m\u001b[39m'\u001b[39m: n_estimators, \u001b[39m# hparam grid if optimization is needed\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive/Alle_Ordner/Coding/Python/master_thesis/rf008.ipynb#ch0000014?line=40'>41</a>\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mmax_depth\u001b[39m\u001b[39m'\u001b[39m: max_depth,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive/Alle_Ordner/Coding/Python/master_thesis/rf008.ipynb#ch0000014?line=41'>42</a>\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mmin_samples_split\u001b[39m\u001b[39m'\u001b[39m: min_samples_split,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive/Alle_Ordner/Coding/Python/master_thesis/rf008.ipynb#ch0000014?line=42'>43</a>\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mmin_samples_leaf\u001b[39m\u001b[39m'\u001b[39m: min_samples_leaf,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive/Alle_Ordner/Coding/Python/master_thesis/rf008.ipynb#ch0000014?line=43'>44</a>\u001b[0m             }\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive/Alle_Ordner/Coding/Python/master_thesis/rf008.ipynb#ch0000014?line=45'>46</a>\u001b[0m \u001b[39mfor\u001b[39;00m run_id, predictors \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(preds):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/OneDrive/Alle_Ordner/Coding/Python/master_thesis/rf008.ipynb#ch0000014?line=46'>47</a>\u001b[0m     rf008\u001b[39m.\u001b[39;49mrun(season, predictors, percentile, station_names, \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive/Alle_Ordner/Coding/Python/master_thesis/rf008.ipynb#ch0000014?line=47'>48</a>\u001b[0m     clf, hparam_grid, optimizer, \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive/Alle_Ordner/Coding/Python/master_thesis/rf008.ipynb#ch0000014?line=48'>49</a>\u001b[0m     run_id, model_run, \n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/OneDrive/Alle_Ordner/Coding/Python/master_thesis/rf008.ipynb#ch0000014?line=49'>50</a>\u001b[0m     k\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, n_iter\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, is_optimized\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, is_scaled\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[1;32md:\\OneDrive\\Alle_Ordner\\Coding\\Python\\master_thesis\\models\\random_forest\\rf008.py:100\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(season, predictors, percentile, station_names, clf, hparam_grid, optimizer, run_id, model_run, k, n_iter, is_optimized, is_scaled)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAdd predictor \u001b[39m\u001b[39m{\u001b[39;00mpredictor\u001b[39m}\u001b[39;00m\u001b[39m to model input features\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     99\u001b[0m era5_predictor \u001b[39m=\u001b[39m data_loader\u001b[39m.\u001b[39mload_daymean_era5(range_of_years, subregion, season, predictor, preprocess)\n\u001b[1;32m--> 100\u001b[0m X_, Y_, t_ \u001b[39m=\u001b[39m preprocessing\u001b[39m.\u001b[39;49mintersect_time(era5_predictor, gesla_predictand)\n\u001b[0;32m    102\u001b[0m X\u001b[39m.\u001b[39mappend(X_)\n\u001b[0;32m    103\u001b[0m Y\u001b[39m.\u001b[39mappend(Y_)\n",
      "File \u001b[1;32md:\\OneDrive\\Alle_Ordner\\Coding\\Python\\master_thesis\\data\\preprocessing.py:160\u001b[0m, in \u001b[0;36mintersect_time\u001b[1;34m(predictor, predictand)\u001b[0m\n\u001b[0;32m    158\u001b[0m predictor_time \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_datetime(predictor\u001b[39m.\u001b[39mtime\u001b[39m.\u001b[39mvalues)\u001b[39m.\u001b[39mdate\n\u001b[0;32m    159\u001b[0m predictand_time \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mto_datetime(predictand\u001b[39m.\u001b[39mdate_time\u001b[39m.\u001b[39mvalues)\u001b[39m.\u001b[39mdate\n\u001b[1;32m--> 160\u001b[0m predictor \u001b[39m=\u001b[39m predictor\u001b[39m.\u001b[39;49mvalues \u001b[39m# Daily data\u001b[39;00m\n\u001b[0;32m    161\u001b[0m predictand \u001b[39m=\u001b[39m predictand\u001b[39m.\u001b[39mvalues \u001b[39m# Hourly data\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[39m# Choose maximum per day, i.e. if one hour\u001b[39;00m\n\u001b[0;32m    164\u001b[0m \u001b[39m# a day indicates an extreme surge, the whole day \u001b[39;00m\n\u001b[0;32m    165\u001b[0m \u001b[39m# is seen as extreme surge.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Programme\\miniconda3\\envs\\mlpy38\\lib\\site-packages\\xarray\\core\\dataarray.py:642\u001b[0m, in \u001b[0;36mDataArray.values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    633\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m    634\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalues\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[0;32m    635\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    636\u001b[0m \u001b[39m    The array's data as a numpy.ndarray.\u001b[39;00m\n\u001b[0;32m    637\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    640\u001b[0m \u001b[39m    type does not support coercion like this (e.g. cupy).\u001b[39;00m\n\u001b[0;32m    641\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 642\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvariable\u001b[39m.\u001b[39;49mvalues\n",
      "File \u001b[1;32md:\\Programme\\miniconda3\\envs\\mlpy38\\lib\\site-packages\\xarray\\core\\variable.py:512\u001b[0m, in \u001b[0;36mVariable.values\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    509\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m    510\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalues\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    511\u001b[0m     \u001b[39m\"\"\"The variable's data as a numpy.ndarray\"\"\"\u001b[39;00m\n\u001b[1;32m--> 512\u001b[0m     \u001b[39mreturn\u001b[39;00m _as_array_or_item(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data)\n",
      "File \u001b[1;32md:\\Programme\\miniconda3\\envs\\mlpy38\\lib\\site-packages\\xarray\\core\\variable.py:252\u001b[0m, in \u001b[0;36m_as_array_or_item\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_as_array_or_item\u001b[39m(data):\n\u001b[0;32m    239\u001b[0m     \u001b[39m\"\"\"Return the given values as a numpy array, or as an individual item if\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[39m    it's a 0d datetime64 or timedelta64 array.\u001b[39;00m\n\u001b[0;32m    241\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[39m    TODO: remove this (replace with np.asarray) once these issues are fixed\u001b[39;00m\n\u001b[0;32m    251\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 252\u001b[0m     data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(data)\n\u001b[0;32m    253\u001b[0m     \u001b[39mif\u001b[39;00m data\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    254\u001b[0m         \u001b[39mif\u001b[39;00m data\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mM\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32md:\\Programme\\miniconda3\\envs\\mlpy38\\lib\\site-packages\\xarray\\core\\indexing.py:552\u001b[0m, in \u001b[0;36mMemoryCachedArray.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 552\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ensure_cached()\n\u001b[0;32m    553\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39masarray(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39marray, dtype\u001b[39m=\u001b[39mdtype)\n",
      "File \u001b[1;32md:\\Programme\\miniconda3\\envs\\mlpy38\\lib\\site-packages\\xarray\\core\\indexing.py:549\u001b[0m, in \u001b[0;36mMemoryCachedArray._ensure_cached\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    547\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_ensure_cached\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    548\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39marray, NumpyIndexingAdapter):\n\u001b[1;32m--> 549\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39marray \u001b[39m=\u001b[39m NumpyIndexingAdapter(np\u001b[39m.\u001b[39;49masarray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49marray))\n",
      "File \u001b[1;32md:\\Programme\\miniconda3\\envs\\mlpy38\\lib\\site-packages\\xarray\\core\\indexing.py:522\u001b[0m, in \u001b[0;36mCopyOnWriteArray.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    521\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m--> 522\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49masarray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49marray, dtype\u001b[39m=\u001b[39;49mdtype)\n",
      "File \u001b[1;32md:\\Programme\\miniconda3\\envs\\mlpy38\\lib\\site-packages\\xarray\\core\\indexing.py:423\u001b[0m, in \u001b[0;36mLazilyIndexedArray.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    422\u001b[0m     array \u001b[39m=\u001b[39m as_indexable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39marray)\n\u001b[1;32m--> 423\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49masarray(array[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey], dtype\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[1;32md:\\Programme\\miniconda3\\envs\\mlpy38\\lib\\site-packages\\xarray\\coding\\variables.py:70\u001b[0m, in \u001b[0;36m_ElementwiseFunctionArray.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49marray)\n",
      "File \u001b[1;32md:\\Programme\\miniconda3\\envs\\mlpy38\\lib\\site-packages\\xarray\\coding\\variables.py:217\u001b[0m, in \u001b[0;36m_scale_offset_decoding\u001b[1;34m(data, scale_factor, add_offset, dtype)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_scale_offset_decoding\u001b[39m(data, scale_factor, add_offset, dtype):\n\u001b[1;32m--> 217\u001b[0m     data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49marray(data, dtype\u001b[39m=\u001b[39;49mdtype, copy\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mif\u001b[39;00m scale_factor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    219\u001b[0m         data \u001b[39m*\u001b[39m\u001b[39m=\u001b[39m scale_factor\n",
      "File \u001b[1;32md:\\Programme\\miniconda3\\envs\\mlpy38\\lib\\site-packages\\xarray\\coding\\variables.py:70\u001b[0m, in \u001b[0;36m_ElementwiseFunctionArray.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49marray)\n",
      "File \u001b[1;32md:\\Programme\\miniconda3\\envs\\mlpy38\\lib\\site-packages\\xarray\\coding\\variables.py:137\u001b[0m, in \u001b[0;36m_apply_mask\u001b[1;34m(data, encoded_fill_values, decoded_fill_value, dtype)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_apply_mask\u001b[39m(\n\u001b[0;32m    134\u001b[0m     data: np\u001b[39m.\u001b[39mndarray, encoded_fill_values: \u001b[39mlist\u001b[39m, decoded_fill_value: Any, dtype: Any\n\u001b[0;32m    135\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[0;32m    136\u001b[0m     \u001b[39m\"\"\"Mask all matching values in a NumPy arrays.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(data, dtype\u001b[39m=\u001b[39;49mdtype)\n\u001b[0;32m    138\u001b[0m     condition \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    139\u001b[0m     \u001b[39mfor\u001b[39;00m fv \u001b[39min\u001b[39;00m encoded_fill_values:\n",
      "File \u001b[1;32md:\\Programme\\miniconda3\\envs\\mlpy38\\lib\\site-packages\\xarray\\core\\indexing.py:423\u001b[0m, in \u001b[0;36mLazilyIndexedArray.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    421\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    422\u001b[0m     array \u001b[39m=\u001b[39m as_indexable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39marray)\n\u001b[1;32m--> 423\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39masarray(array[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey], dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32md:\\Programme\\miniconda3\\envs\\mlpy38\\lib\\site-packages\\xarray\\backends\\netCDF4_.py:93\u001b[0m, in \u001b[0;36mNetCDF4ArrayWrapper.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[1;32m---> 93\u001b[0m     \u001b[39mreturn\u001b[39;00m indexing\u001b[39m.\u001b[39;49mexplicit_indexing_adapter(\n\u001b[0;32m     94\u001b[0m         key, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mshape, indexing\u001b[39m.\u001b[39;49mIndexingSupport\u001b[39m.\u001b[39;49mOUTER, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem\n\u001b[0;32m     95\u001b[0m     )\n",
      "File \u001b[1;32md:\\Programme\\miniconda3\\envs\\mlpy38\\lib\\site-packages\\xarray\\core\\indexing.py:712\u001b[0m, in \u001b[0;36mexplicit_indexing_adapter\u001b[1;34m(key, shape, indexing_support, raw_indexing_method)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[39m\"\"\"Support explicit indexing by delegating to a raw indexing method.\u001b[39;00m\n\u001b[0;32m    691\u001b[0m \n\u001b[0;32m    692\u001b[0m \u001b[39mOuter and/or vectorized indexers are supported by indexing a second time\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[39mIndexing result, in the form of a duck numpy-array.\u001b[39;00m\n\u001b[0;32m    710\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    711\u001b[0m raw_key, numpy_indices \u001b[39m=\u001b[39m decompose_indexer(key, shape, indexing_support)\n\u001b[1;32m--> 712\u001b[0m result \u001b[39m=\u001b[39m raw_indexing_method(raw_key\u001b[39m.\u001b[39;49mtuple)\n\u001b[0;32m    713\u001b[0m \u001b[39mif\u001b[39;00m numpy_indices\u001b[39m.\u001b[39mtuple:\n\u001b[0;32m    714\u001b[0m     \u001b[39m# index the loaded np.ndarray\u001b[39;00m\n\u001b[0;32m    715\u001b[0m     result \u001b[39m=\u001b[39m NumpyIndexingAdapter(np\u001b[39m.\u001b[39masarray(result))[numpy_indices]\n",
      "File \u001b[1;32md:\\Programme\\miniconda3\\envs\\mlpy38\\lib\\site-packages\\xarray\\backends\\netCDF4_.py:106\u001b[0m, in \u001b[0;36mNetCDF4ArrayWrapper._getitem\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    104\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdatastore\u001b[39m.\u001b[39mlock:\n\u001b[0;32m    105\u001b[0m         original_array \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_array(needs_lock\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m--> 106\u001b[0m         array \u001b[39m=\u001b[39m getitem(original_array, key)\n\u001b[0;32m    107\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n\u001b[0;32m    108\u001b[0m     \u001b[39m# Catch IndexError in netCDF4 and return a more informative\u001b[39;00m\n\u001b[0;32m    109\u001b[0m     \u001b[39m# error message.  This is most often called when an unsorted\u001b[39;00m\n\u001b[0;32m    110\u001b[0m     \u001b[39m# indexer is used before the data is loaded from disk.\u001b[39;00m\n\u001b[0;32m    111\u001b[0m     msg \u001b[39m=\u001b[39m (\n\u001b[0;32m    112\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe indexing operation you are attempting to perform \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    113\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mis not valid on netCDF4.Variable object. Try loading \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    114\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myour data into memory first by calling .load().\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    115\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#---\n",
    "# Initialize\n",
    "#---\n",
    "from models.random_forest import rf008\n",
    "\n",
    "model_run = \"rf008\"\n",
    "season = \"winter\" # [\"winter\", \"autumn\",] \n",
    "percentile = 0.95 # [0.95, 0.99,] \n",
    "station_names = [\"hanko-han-fin-cmems\",]\n",
    "preds = [\n",
    "    [\"sp\", \"u10\",], # run_id 0\n",
    "    [\"sp\", \"tp\", \"u10\",], # run_id 1,...\n",
    "    [\"sp\", \"tp\",],\n",
    "    [\"tp\",\"u10\"],\n",
    "    [\"sp\", \"tp\", \"u10\", \"v10\"],\n",
    "]\n",
    "\n",
    "clf = RandomForestClassifier\n",
    "optimizer = \"RandomSearchCV\" #[\"RandomSearchCV\", \"GridSearchCV\"]\n",
    "\n",
    "#---\n",
    "# Build Hyperparameter Grid to optimize from.\n",
    "# For this run, use exactly the same as in rf004 to see if \n",
    "# scaling of predictor data leads to any changes.\n",
    "#---\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 0, stop = 1000, num = 10)]\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(5, 55, num = 5)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Create the random grid\n",
    "hparam_grid = {'n_estimators': n_estimators, # hparam grid if optimization is needed\n",
    "            'max_depth': max_depth,\n",
    "            'min_samples_split': min_samples_split,\n",
    "            'min_samples_leaf': min_samples_leaf,\n",
    "            }\n",
    "\n",
    "for run_id, predictors in enumerate(preds):\n",
    "    rf008.run(season, predictors, percentile, station_names, \n",
    "    clf, hparam_grid, optimizer, \n",
    "    run_id, model_run, \n",
    "    k=3, n_iter=None, is_optimized=True, is_scaled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used model: RandomForestClassifier(class_weight='balanced', criterion='entropy',\n",
      "                       max_depth=4, min_samples_leaf=2, n_estimators=1,\n",
      "                       oob_score=True, random_state=0)\n",
      "Fit model\n",
      "Testscore: 0.9333333333333333\n",
      "Trainscore: 0.9459459459459459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programme\\miniconda3\\envs\\mlpy38\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:560: UserWarning: Some inputs do not have OOB scores. This probably means too few trees were used to compute any reliable OOB estimates.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "#---\n",
    "# Goal: Check if clf = classifier of my choice is adjustable or not like \n",
    "# model = clf(param_grid)\n",
    "# Use best_params from modelfit.optimize_hyperparameter output to apply params to model.\n",
    "#---\n",
    "\n",
    "#---\n",
    "# Load example data \n",
    "#---\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "with zipfile.ZipFile('../courses/machine_learning/Kursmaterialien.zip', 'r') as source:\n",
    "    with source.open('Kursmaterialien/Abschnitt 26 - Entscheidungsbaeume/classification.csv') as file:\n",
    "        df = pd.read_csv(file, low_memory=False)\n",
    "\n",
    "df.head()\n",
    "\n",
    "#- Train & Test split\n",
    "X = df.drop('success', axis = 1).values #- Copies DF\n",
    "y = df['success'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.25)\n",
    "\n",
    "# Example parameters for model fit\n",
    "#---\n",
    "clf = RandomForestClassifier\n",
    "best_params = {\n",
    "    \"n_estimators\" : 1,\n",
    "    \"max_depth\" : 4, \n",
    "    \"criterion\" : \"entropy\",\n",
    "    \"min_samples_leaf\" : 2, \n",
    "    \"min_samples_split\" : 2, \n",
    "    \"random_state\" : 0, \n",
    "    \"class_weight\" : \"balanced\",\n",
    "    \"oob_score\" : True,\n",
    "}\n",
    "\n",
    "#---\n",
    "# Fit the model\n",
    "#---\n",
    "model1 = clf(**best_params) # One can set parameters afterwards via model.set_params()\n",
    "\n",
    "model2 = RandomForestClassifier(criterion='gini',\n",
    "n_estimators=best_params[\"n_estimators\"], #- nTrees \n",
    "max_depth=best_params[\"max_depth\"], \n",
    "min_samples_leaf=best_params[\"min_samples_leaf\"],\n",
    "min_samples_split=best_params[\"min_samples_split\"],\n",
    "random_state=0, # To compare results when changing hyperparameters\n",
    "class_weight=\"balanced\",\n",
    "oob_score=True,\n",
    ") \n",
    "\n",
    "print(f\"Used model: {model1}\")\n",
    "print(\"Fit model\")\n",
    "model1.fit(X_train, y_train)\n",
    "print(f\"Testscore: {model1.score(X_test, y_test)}\")\n",
    "print(f\"Trainscore: {model1.score(X_train, y_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m\n",
      "\u001b[0mmodelfit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize_hyperparameter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mparam_grid\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "Description: \n",
      "    Return best hyperparameters for a model based on chosen optimizer\n",
      "Parameters:\n",
      "    X_train (): Predictor train data\n",
      "    y_train (): Predictand train data\n",
      "    clf (): Base Model\n",
      "    optimizer (): GridSearchCV or RandomizedSearchCV\n",
      "    param_grid (dict): Dictionary with hyperparameter ranges\n",
      "    k (int): k-fold Cross-Validation\n",
      "    n_iter (int): Number of combinations used for RandomizedSearchCV (Defaults:None)\n",
      "    n_jobs (int): Number of processor used. (Defaults:-1, e.g. all processors)\n",
      "\u001b[1;31mFile:\u001b[0m      d:\\onedrive\\alle_ordner\\coding\\python\\master_thesis\\models\\modelfit.py\n",
      "\u001b[1;31mType:\u001b[0m      function\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from models import modelfit\n",
    "\n",
    "type(LogisticRegression())\n",
    "modelfit.optimize_hyperparameter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---\n",
    "# Scale data\n",
    "#---\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "s = StandardScaler()\n",
    "s.fit(X_train)\n",
    "X_train = s.transform(X_train)\n",
    "X_test = s.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---\n",
    "# Learning Curve\n",
    "#---\n",
    "\n",
    "#- Train & Test split\n",
    "X = df.drop('success', axis = 1).values #- Copies DF\n",
    "y = df['success'].values\n",
    "\n",
    "#- Plot Learning Curve\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "\n",
    "X, y = shuffle(X, y) #- random selection of data. Good if you dont know if data is ordered\n",
    "train_sizes_abs, train_scores, test_scores = learning_curve(LogisticRegression(), X, y)\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_sizes_abs, np.mean(train_scores, axis = 1)) #- learning curve macht automatisch k-fold crossvalidation. deswegen mean\n",
    "plt.plot(train_sizes_abs, np.mean(test_scores, axis = 1)) #- learning curve macht automatisch k-fold crossvalidation. deswegen mean\n",
    "\n",
    "plt.show()\n",
    "\n",
    "#- Note: Do this more often to get mean. Sometimes Curves look weird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---\n",
    "# Pipeline GridSearchCV: \n",
    "# Add this to modelfit.py?\n",
    "#---\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC \n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"svc\", SVC()),\n",
    "])\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "clf = GridSearchCV(pipeline, param_grid = {\n",
    "    \"svc__C\": [0.001, 0.01, 0.1, 1, 10,],\n",
    "    \"svc__gamma\": [0.001, 0.01, 0.1, 1, 10,],\n",
    "})\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(clf.best_params_)\n",
    "\n",
    "print(clf.score(X_validation, y_validation)) # Accuracy on basis of test data\n",
    "\n",
    "print(clf.best_score_) # Accuracy based on k-fold cross-validation"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dbda0df39e8d9f08c77def502e386bb54b10e33e74098d5a47640c70cb49662a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('mlpy38')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
